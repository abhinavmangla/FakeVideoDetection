{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CRNN\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.transform = transform\n",
    "        #self.frames = frames\n",
    "        self.folders = data_path\n",
    "        self.frames = 10 #For our case since we are computing 10 frames always\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.folders))\n",
    "\n",
    "    def read_images(self, data_path, use_transform):\n",
    "        X = []\n",
    "        for i in os.listdir(data_path):\n",
    "            #print(\"file name is \",i)\n",
    "            image = Image.open(os.path.join(data_path,i))\n",
    "            \n",
    "            #print(image.shape)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "                #print(image.size)\n",
    "            image = torch.from_numpy(np.asarray(image))\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.folders,os.listdir(self.folders)[index])\n",
    "              \n",
    "        # Load data\n",
    "        X = self.read_images(data_path, self.transform)                     # (input) spatial images\n",
    "        \n",
    "        y = np.ones(self.frames)\n",
    "        if 'orig' in data_path:\n",
    "            y = np.zeros(self.frames)\n",
    "        # print(X.shape)\n",
    "        return X, torch.from_numpy(y).type(torch.LongTensor)  #torch.from_numpy(y).type(torch.LongTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/chinmay/datatset/train/'\n",
    "train_data = Dataset_CRNN(train_path, transform=TRANSFORM_IMG)\n",
    "# for step, (x, y) in enumerate(data):\n",
    "#     print(x.shape)\n",
    "val_path = '/home/chinmay/datatset/val/'\n",
    "val_data = Dataset_CRNN(val_path, transform=TRANSFORM_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_output_shape(num_of_maxpool_2,shape):\n",
    "    return int(shape/(2**num_of_maxpool_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(new_output_shape(4,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Meso4(nn.Module):\n",
    "#     def __init__(self,in_channel=3, img_shape = 256, number_of_classes=1):\n",
    "#         super(Meso4,self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channel,8, kernel_size=(3,3), stride = 1, padding= 1)\n",
    "#         self.batch_norm_1 = nn.BatchNorm2d(8)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(8,8, kernel_size=(5,5),stride=1, padding=2)\n",
    "#         self.batch_norm_2 = nn.BatchNorm2d(8)\n",
    "#         self.conv3 = nn.Conv2d(8,16, kernel_size=(5,5),stride=1, padding=2)\n",
    "#         self.batch_norm_3 = nn.BatchNorm2d(16)\n",
    "#         self.conv4 = nn.Conv2d(16,16, kernel_size=(5,5),stride=1, padding=2)\n",
    "#         self.batch_norm_4 = nn.BatchNorm2d(16)\n",
    "#         self.max_pool_2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         self.max_pool_4 = nn.MaxPool2d(kernel_size=(4, 4), stride=4)\n",
    "#         out_shape = new_output_shape(num_of_maxpool_2=4, shape = img_shape)\n",
    "#         self.fc_conv = nn.Conv2d(16,number_of_classes,kernel_size=(out_shape,out_shape))\n",
    "#         self.dropout = nn.Dropout2d(p=0.2)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self,x_3d):\n",
    "#         cnn_embed_seq = []\n",
    "#         x_3d = x_3d.permute(0,1,4,2,3)# Required to match shapes\n",
    "#         x_3d = x_3d.type(torch.cuda.FloatTensor) #Converting to Float Tensor from Byte Tensor\n",
    "#         for t in range(x_3d.size(1)):\n",
    "#             x = self.conv1(x_3d[:, t, :, :, :])\n",
    "#             x = self.batch_norm_1(x)\n",
    "#             x = self.relu(x)\n",
    "#             # x = self.dropout(x) Avoid in the first layer\n",
    "#             # Segment 2\n",
    "#             x = self.max_pool_2(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.batch_norm_2(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.dropout(x)\n",
    "#             # Segment 3\n",
    "#             x = self.max_pool_2(x)\n",
    "#             x = self.conv3(x)\n",
    "#             x = self.batch_norm_3(x)\n",
    "#             x = self.relu(x)\n",
    "#             #x = self.dropout(x)\n",
    "#             # Segment 4\n",
    "#             x = self.max_pool_2(x)\n",
    "#             x = self.conv4(x)\n",
    "#             x = self.batch_norm_4(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.dropout(x)\n",
    "#             # Going for the last layer\n",
    "#             x = self.max_pool_2(x)\n",
    "#             x = self.fc_conv(x)\n",
    "#             #print(\"Shape of x is {}\".format(x.shape))\n",
    "#             x = self.sigmoid(x)\n",
    "#             x = x.view(x.shape[0], -1)\n",
    "            \n",
    "#             cnn_embed_seq.append(x)\n",
    "#         cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "#         return cnn_embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 40\n",
    "log_interval = 10\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = data.DataLoader(train_data, **params)\n",
    "valid_loader = data.DataLoader(val_data, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meso4(\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batch_norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (max_pool_2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (max_pool_4): MaxPool2d(kernel_size=(4, 4), stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_conv): Conv2d(16, 2, kernel_size=(16, 16), stride=(1, 1))\n",
       "  (dropout): Dropout2d(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Meso4 import Meso4\n",
    "model = Meso4()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(numpy_array = []): #This is expected to take an array of array. So,\n",
    "    #print(\"Input array is {}\".format(numpy_array))\n",
    "    output = []\n",
    "    for array in numpy_array:\n",
    "        counts = np.bincount(array)\n",
    "        output.append(np.argmax(counts))\n",
    "    return torch.from_numpy(np.asarray(output)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train = SummaryWriter('/home/chinmay/training-results/mesonet_final/train')\n",
    "writer_test = SummaryWriter('/home/chinmay/training-results/mesonet_final/test')\n",
    "save_model_path = \"/home/chinmay/model_weights/MesoNet_final/MesoNet40\"\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train() #Put the model in training mode\n",
    "    losses = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    scores = []\n",
    "    #single_iter_loss = []\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)   # output has dim = (batch,frames, number of classes)\n",
    "        #print(output)\n",
    "        #print(y.shape)\n",
    "        loss_train = 0\n",
    "        for items in range(output.shape[1]):\n",
    "            loss = loss_function(output[:,items,:], y[:,items])\n",
    "            loss_train = loss_train + loss.item()\n",
    "            #single_iter_loss.append(loss)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        #print(\"backward update\")\n",
    "        #loss_batch = np.sum(single_iter_loss)\n",
    "        #loss_batch.backward(retain_graph=True) #Possible improvement to increase speed\n",
    "        #optimizer.step()\n",
    "        \n",
    "        losses.append(loss_train)\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 2)[1]  # y_pred != output\n",
    "        # We take the modal value of all predictions. So,\n",
    "        y = find_median(y)\n",
    "        y_pred = find_median(y_pred)\n",
    "        #print(\"filename is {}\".format(file_name))\n",
    "        #print(\"prediction is {}\".format(y_pred))\n",
    "        \n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "        #print(step_score)\n",
    "        \n",
    "\n",
    "        # show information\n",
    "                 \n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss_train, 100 * step_score))\n",
    "        \n",
    "    # we should sum all these losses and then divide by the training batch size\n",
    "    losses_scalar = np.sum(losses)/ len(train_loader.dataset) \n",
    "    return losses_scalar, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            #print(\"Output shape is {}\".format(output.shape))\n",
    "            #print(\"Expected shape is {}\".format(y.shape))\n",
    "            for items in range(output.shape[1]):\n",
    "                loss = loss_function(output[:,items,:], y[:,items])\n",
    "                test_loss += loss.item()                 # sum up batch loss\n",
    "            #Predict value\n",
    "            y_pred = torch.max(output, 2)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "            # Replace the value with the median value\n",
    "            y_pred = find_median(y_pred)\n",
    "            y = find_median(y)\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, 'meso_df_2_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'meso_df_2_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    \n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preload the model weights\n",
    "#save_model_path = \"/home/chinmay/model_weights/MesoNet/MesoNet40\"\n",
    "# model_name = \"3dcnn_epoch10.pth\"\n",
    "# PATH = save_model_path+model_name\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, learning_rate, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 20 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 15))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [320/1408 (23%)]\tLoss: 8.725730, Accu: 43.75%\n",
      "Train Epoch: 1 [640/1408 (45%)]\tLoss: 6.858820, Accu: 56.25%\n",
      "Train Epoch: 1 [960/1408 (68%)]\tLoss: 7.156719, Accu: 50.00%\n",
      "Train Epoch: 1 [1280/1408 (91%)]\tLoss: 6.903403, Accu: 56.25%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2318, Accuracy: 51.00%\n",
      "\n",
      "Train Epoch: 2 [320/1408 (23%)]\tLoss: 6.747351, Accu: 65.62%\n",
      "Train Epoch: 2 [640/1408 (45%)]\tLoss: 6.906965, Accu: 59.38%\n",
      "Train Epoch: 2 [960/1408 (68%)]\tLoss: 6.904117, Accu: 50.00%\n",
      "Train Epoch: 2 [1280/1408 (91%)]\tLoss: 6.920582, Accu: 56.25%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2314, Accuracy: 50.33%\n",
      "\n",
      "Train Epoch: 3 [320/1408 (23%)]\tLoss: 6.932699, Accu: 53.12%\n",
      "Train Epoch: 3 [640/1408 (45%)]\tLoss: 7.274980, Accu: 46.88%\n",
      "Train Epoch: 3 [960/1408 (68%)]\tLoss: 6.859964, Accu: 50.00%\n",
      "Train Epoch: 3 [1280/1408 (91%)]\tLoss: 6.806368, Accu: 62.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2296, Accuracy: 52.33%\n",
      "\n",
      "Train Epoch: 4 [320/1408 (23%)]\tLoss: 6.914430, Accu: 53.12%\n",
      "Train Epoch: 4 [640/1408 (45%)]\tLoss: 6.591485, Accu: 68.75%\n",
      "Train Epoch: 4 [960/1408 (68%)]\tLoss: 7.895416, Accu: 28.12%\n",
      "Train Epoch: 4 [1280/1408 (91%)]\tLoss: 7.398129, Accu: 46.88%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2270, Accuracy: 52.33%\n",
      "\n",
      "Train Epoch: 5 [320/1408 (23%)]\tLoss: 6.745839, Accu: 59.38%\n",
      "Train Epoch: 5 [640/1408 (45%)]\tLoss: 6.975715, Accu: 50.00%\n",
      "Train Epoch: 5 [960/1408 (68%)]\tLoss: 6.512687, Accu: 71.88%\n",
      "Train Epoch: 5 [1280/1408 (91%)]\tLoss: 5.820272, Accu: 75.00%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2291, Accuracy: 55.00%\n",
      "\n",
      "Train Epoch: 6 [320/1408 (23%)]\tLoss: 8.084056, Accu: 56.25%\n",
      "Train Epoch: 6 [640/1408 (45%)]\tLoss: 7.031417, Accu: 40.62%\n",
      "Train Epoch: 6 [960/1408 (68%)]\tLoss: 6.320399, Accu: 71.88%\n",
      "Train Epoch: 6 [1280/1408 (91%)]\tLoss: 7.373702, Accu: 56.25%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2160, Accuracy: 62.67%\n",
      "\n",
      "Train Epoch: 7 [320/1408 (23%)]\tLoss: 6.536902, Accu: 62.50%\n",
      "Train Epoch: 7 [640/1408 (45%)]\tLoss: 6.069192, Accu: 71.88%\n",
      "Train Epoch: 7 [960/1408 (68%)]\tLoss: 5.890566, Accu: 71.88%\n",
      "Train Epoch: 7 [1280/1408 (91%)]\tLoss: 5.990020, Accu: 71.88%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2377, Accuracy: 56.67%\n",
      "\n",
      "Train Epoch: 8 [320/1408 (23%)]\tLoss: 6.755695, Accu: 50.00%\n",
      "Train Epoch: 8 [640/1408 (45%)]\tLoss: 6.158325, Accu: 75.00%\n",
      "Train Epoch: 8 [960/1408 (68%)]\tLoss: 6.301317, Accu: 59.38%\n",
      "Train Epoch: 8 [1280/1408 (91%)]\tLoss: 6.350803, Accu: 65.62%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.2120, Accuracy: 64.00%\n",
      "\n",
      "Train Epoch: 9 [320/1408 (23%)]\tLoss: 5.951499, Accu: 78.12%\n",
      "Train Epoch: 9 [640/1408 (45%)]\tLoss: 7.658302, Accu: 50.00%\n",
      "Train Epoch: 9 [960/1408 (68%)]\tLoss: 5.629272, Accu: 75.00%\n",
      "Train Epoch: 9 [1280/1408 (91%)]\tLoss: 5.360786, Accu: 81.25%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.1947, Accuracy: 70.00%\n",
      "\n",
      "Train Epoch: 10 [320/1408 (23%)]\tLoss: 6.251065, Accu: 65.62%\n",
      "Train Epoch: 10 [640/1408 (45%)]\tLoss: 6.101721, Accu: 65.62%\n",
      "Train Epoch: 10 [960/1408 (68%)]\tLoss: 6.070902, Accu: 68.75%\n",
      "Train Epoch: 10 [1280/1408 (91%)]\tLoss: 6.750685, Accu: 62.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.1909, Accuracy: 74.67%\n",
      "\n",
      "Train Epoch: 11 [320/1408 (23%)]\tLoss: 6.844443, Accu: 59.38%\n",
      "Train Epoch: 11 [640/1408 (45%)]\tLoss: 4.957319, Accu: 84.38%\n",
      "Train Epoch: 11 [960/1408 (68%)]\tLoss: 4.992753, Accu: 81.25%\n",
      "Train Epoch: 11 [1280/1408 (91%)]\tLoss: 4.492388, Accu: 90.62%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.1838, Accuracy: 75.33%\n",
      "\n",
      "Train Epoch: 12 [320/1408 (23%)]\tLoss: 5.194449, Accu: 78.12%\n",
      "Train Epoch: 12 [640/1408 (45%)]\tLoss: 6.280506, Accu: 65.62%\n",
      "Train Epoch: 12 [960/1408 (68%)]\tLoss: 4.682839, Accu: 81.25%\n",
      "Train Epoch: 12 [1280/1408 (91%)]\tLoss: 4.801245, Accu: 84.38%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.1866, Accuracy: 74.00%\n",
      "\n",
      "Train Epoch: 13 [320/1408 (23%)]\tLoss: 5.388976, Accu: 68.75%\n",
      "Train Epoch: 13 [640/1408 (45%)]\tLoss: 4.188584, Accu: 87.50%\n",
      "Train Epoch: 13 [960/1408 (68%)]\tLoss: 5.304220, Accu: 75.00%\n",
      "Train Epoch: 13 [1280/1408 (91%)]\tLoss: 5.736027, Accu: 71.88%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.1889, Accuracy: 74.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(model, device, optimizer, valid_loader)\n",
    "    # Reduce learning-rate by a factor of 1/10 after every 10 epochs\n",
    "    # avoid this step as Adam is being used\n",
    "    #adjust_learning_rate(optimizer=optimizer, learning_rate=learning_rate, epoch=epoch)\n",
    "    \n",
    "    # save results\n",
    "    writer_train.add_scalar('loss',train_losses,epoch+1)\n",
    "    writer_train.add_scalar('score',train_scores,epoch+1)\n",
    "    writer_test.add_scalar('loss',epoch_test_loss,epoch+1)\n",
    "    writer_test.add_scalar('score',epoch_test_score,epoch+1)\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "    #Empty the cache\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_loss = []\n",
    "for single_epoch in epoch_train_losses:\n",
    "    mean_train_loss.append(single_epoch/len(train_loader.dataset))\n",
    "# Same for Train-score\n",
    "# mean_train_score = []\n",
    "# for single_epoch in epoch_train_scores:\n",
    "#     mean_train_score.append(single_epoch/len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "e = np.arange(1, epochs + 1)\n",
    "plt.plot(e,epoch_train_score)\n",
    "plt.plot(e,epoch_test_scores)\n",
    "plt.legend(['train scores', 'validation scores'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "e = np.arange(1, epochs + 1)\n",
    "plt.plot(e,mean_train_loss)\n",
    "plt.plot(e,epoch_test_losses)\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"/home/chinmay/model_weights/MesoNet/\"\n",
    "torch.save(model.state_dict(), os.path.join(save_model_path, 'meso_inception_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'meso_inception_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, a=1, b=4, c=4, d=1):\n",
    "        super(InceptionLayer, self).__init__()\n",
    "        self.basic_conv_a = nn.Conv2d(in_channels=in_channels, out_channels=a, kernel_size=1)\n",
    "        self.bn_a = nn.BatchNorm2d(a, eps=0.001)\n",
    "        # In all these cases, network passes through layer conv1X1 first\n",
    "        self.basic_conv_b = nn.Conv2d(in_channels=a, out_channels=b, kernel_size=3, padding=1)\n",
    "        self.bn_b = nn.BatchNorm2d(b, eps=0.001)\n",
    "        self.basic_conv_c = nn.Conv2d(in_channels=a, out_channels=c, kernel_size=3, padding=2, dilation=2)\n",
    "        self.bn_c = nn.BatchNorm2d(c, eps=0.001)\n",
    "        self.basic_conv_d = nn.Conv2d(in_channels=a, out_channels=d, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn_d = nn.BatchNorm2d(d, eps=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1 = self.basic_conv_a(x)\n",
    "        branch1 = F.relu(self.bn_a(branch1), inplace=True)\n",
    "        \n",
    "        branch3_d0 = self.basic_conv_a(x)\n",
    "        branch3_d0 = self.basic_conv_b(branch3_d0)\n",
    "        branch3_d0 = F.relu(self.bn_b(branch3_d0), inplace=True)\n",
    "\n",
    "        branch3_d1 = self.basic_conv_a(x)\n",
    "        branch3_d1 = self.basic_conv_c(branch3_d1)\n",
    "        branch3_d1 = F.relu(self.bn_c(branch3_d1), inplace=True)\n",
    "        \n",
    "        branch3_d2 = self.basic_conv_a(x)\n",
    "        branch3_d2 = self.basic_conv_d(branch3_d2)\n",
    "        branch3_d2 = F.relu(self.bn_d(branch3_d2), inplace=True)\n",
    "        \n",
    "        outputs = [branch1, branch3_d0, branch3_d1, branch3_d2]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_output_shape(num_of_maxpool_2,shape):\n",
    "    return int(shape/(2**num_of_maxpool_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meso_Incepption_v2(nn.Module):\n",
    "    def __init__(self, in_channels=3, img_shape=256, number_of_classes=1):\n",
    "        super(Meso_Incepption_v2,self).__init__()\n",
    "        self.inception_layer_1 = InceptionLayer(in_channels, a=1, b=4, c=4, d=1)\n",
    "        self.inception_layer_2 = InceptionLayer(in_channels=10, a=1, b=4, c=4, d=2) #First layer had 10 channels\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.max_pool_4 = nn.MaxPool2d(kernel_size=(4, 4), stride=4)\n",
    "        self.conv_5_1 = nn.Conv2d(in_channels=11, out_channels=16, kernel_size=5, padding=2)#Sum of channels\n",
    "        self.conv_5_2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, padding=2)#Sum of channels\n",
    "        final_h_w = new_output_shape(num_of_maxpool_2=5, shape=img_shape)#We have a MaxPool of 4 included here\n",
    "        self.fc_conv = nn.Conv2d(16,number_of_classes,kernel_size=(final_h_w,final_h_w))\n",
    "        \n",
    "    def forward(self,x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        x_3d = x_3d.permute(0,1,4,2,3)# Required to match shapes\n",
    "        x_3d = x_3d.type(torch.cuda.FloatTensor) #Converting to Float Tensor from Byte Tensor\n",
    "        for t in range(x_3d.size(1)):\n",
    "            x1 = self.inception_layer_1(x_3d[:, t, :, :, :])\n",
    "            x1 = self.max_pool_2(x1)\n",
    "            # Second layer\n",
    "            x2 = self.inception_layer_2(x1)\n",
    "            x2 = self.max_pool_2(x2)\n",
    "            # Third layer\n",
    "            x3 = self.conv_5_1(x2)\n",
    "            x3 = F.relu(self.max_pool_2(x3), inplace=True)\n",
    "            # Fourth layer\n",
    "            x4 = self.conv_5_2(x3)\n",
    "            x4 = F.relu(self.max_pool_4(x4), inplace=True)\n",
    "            # Now flatten the layers\n",
    "            x = self.fc_conv(x4)\n",
    "            #print(\"Shape of x is {}\".format(x.shape))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            \n",
    "            cnn_embed_seq.append(x)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        return cnn_embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "model = Meso_Incepption_v2()\n",
    "model.cuda()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
