{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import torch.utils.data as utils\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"/home/rajanie/models/RNN_ckpt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class Dataload_RNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.transform = transform\n",
    "        self.folders = data_path\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.folders))\n",
    "\n",
    "    def read_images(self, data_path, use_transform):\n",
    "        X = []\n",
    "        for i in os.listdir(data_path):\n",
    "            # print(\"file name is \",i)\n",
    "            image = Image.open(os.path.join(data_path, i))\n",
    "\n",
    "            # print(image.shape)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "                # print(image.size)\n",
    "            image = torch.from_numpy(np.asarray(image))\n",
    "            X.append(image)\n",
    "        # print(X)\n",
    "        # X = np.array(X)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        # print(\"index passed is \",index)\n",
    "        # print(self.folders)\n",
    "        data_path = os.path.join(self.folders, os.listdir(self.folders)[index])\n",
    "        # data_path = self.folders+ str(index)\n",
    "        # print(\"Data path is \",data_path)\n",
    "\n",
    "        # Load data\n",
    "        X = self.read_images(data_path, self.transform)  # (input) spatial images\n",
    "\n",
    "        y = 1\n",
    "        if 'orig' in data_path:\n",
    "            y = 0\n",
    "        # print(X.shape)\n",
    "        return X, torch.from_numpy(np.array(y)).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------- end of Dataloaders ---------------------- ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    #transforms.CenterCrop(224),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "train_path = \"/home/chinmay/datatset/train/\"\n",
    "train_data = Dataload_RNN(train_path, transform=TRANSFORM_IMG)\n",
    "val_path = \"/home/chinmay/datatset/val/\"\n",
    "val_data = Dataload_RNN(val_path,  transform=TRANSFORM_IMG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------ CRNN module ---------------------- ##\n",
    "\n",
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "\n",
    "# 2D CNN encoder train from scratch (no transfer learning)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, img_x=90, img_y=120, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        self.CNN_embed_dim = CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
    "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)\n",
    "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "        # self.ch1, self.ch2  = 32, 64,\n",
    "        # self.k1, self.k2 = (5, 5), (3, 3)\n",
    "        # self.s1, self.s2 = (2, 2), (2, 2)\n",
    "        # self.pd1, self.pd2 = (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "\n",
    "        # conv2D output shapes\n",
    "        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\n",
    "        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
    "        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
    "\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
    "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),                      \n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2, padding=self.pd2),\n",
    "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3, padding=self.pd3),\n",
    "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4, padding=self.pd4),\n",
    "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout2d(self.drop_p)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1], self.fc_hidden1)   # fully connected layer, output k classes\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)   # output = CNN embedding latent variables\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        x_3d = x_3d.permute(0, 1, 4, 2, 3)\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # CNNs\n",
    "            \n",
    "            x = self.conv1(x_3d[:, t, :, :, :])\n",
    "            x = self.conv2(x)\n",
    "            # x = self.conv3(x)\n",
    "            # x = self.conv4(x)\n",
    "            x = x.view(x.size(0), -1)           # flatten the output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            # x = F.relu(self.fc2(x))\n",
    "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            # x = self.fc3(x)\n",
    "            x = self.fc2(x)\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "# 2D CNN encoder using ResNet-152 pretrained\n",
    "class ResCNNEncoder(nn.Module):\n",
    "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(ResCNNEncoder, self).__init__()\n",
    "\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        resnet = models.resnet50(pretrained=False)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
    "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        x_3d = x_3d.permute(0, 1, 4, 2, 3)\n",
    "        with torch.no_grad():\n",
    "            for t in range(x_3d.size(1)):\n",
    "                # CNNs\n",
    "                x = self.resnet(x_3d[:, t, :, :, :]) # ResNet\n",
    "                x = x.view(x.size(0), -1)            # flatten output of conv\n",
    "\n",
    "                # FC layers\n",
    "                x = self.bn1(self.fc1(x))\n",
    "                x = F.relu(x)\n",
    "                x = self.bn2(self.fc2(x))\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "                x = self.fc3(x)\n",
    "\n",
    "                cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
    "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,        \n",
    "            num_layers=h_RNN_layers,       \n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "        \n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device, dtype=torch.float), y.to(device)\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        #print(y_pred)\n",
    "        \n",
    "        #step_score = accuracy_score(y, y_pred)\n",
    "        #test_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        step_score = accuracy_score(y, y_pred)\n",
    "\n",
    "        #print(step_score)\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "            \n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(np.mean(losses) ,100 * np.mean(scores)))\n",
    "\n",
    "\n",
    "    return np.mean(losses), np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device,  dtype=torch.float), y.to(device)\n",
    "\n",
    "            output = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "    #test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "   \n",
    "\n",
    "    return test_loss, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1000/1408 (67%)]\tLoss: 0.686578, Accu: 52.00%\n",
      "\n",
      "Train set: Average loss: 0.8575, Accuracy: 51.97%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.7756, Accuracy: 50.00%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [1000/1408 (67%)]\tLoss: 0.710319, Accu: 46.00%\n",
      "\n",
      "Train set: Average loss: 0.7075, Accuracy: 50.17%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6970, Accuracy: 49.00%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [1000/1408 (67%)]\tLoss: 0.692907, Accu: 50.00%\n",
      "\n",
      "Train set: Average loss: 0.6959, Accuracy: 48.33%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6956, Accuracy: 50.00%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "Train Epoch: 4 [1000/1408 (67%)]\tLoss: 0.689876, Accu: 49.00%\n",
      "\n",
      "Train set: Average loss: 0.6953, Accuracy: 53.50%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.7046, Accuracy: 49.67%\n",
      "\n",
      "Train Epoch: 5 [1000/1408 (67%)]\tLoss: 0.694735, Accu: 52.00%\n",
      "\n",
      "Train set: Average loss: 0.6980, Accuracy: 50.90%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6996, Accuracy: 52.67%\n",
      "\n",
      "Train Epoch: 6 [1000/1408 (67%)]\tLoss: 0.694066, Accu: 49.00%\n",
      "\n",
      "Train set: Average loss: 0.6955, Accuracy: 46.87%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6938, Accuracy: 50.33%\n",
      "\n",
      "Epoch 6 model saved!\n",
      "Train Epoch: 7 [1000/1408 (67%)]\tLoss: 0.692804, Accu: 49.00%\n",
      "\n",
      "Train set: Average loss: 0.6936, Accuracy: 51.97%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6935, Accuracy: 52.33%\n",
      "\n",
      "Epoch 7 model saved!\n",
      "Train Epoch: 8 [1000/1408 (67%)]\tLoss: 0.696160, Accu: 45.00%\n",
      "\n",
      "Train set: Average loss: 0.6925, Accuracy: 52.23%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6936, Accuracy: 49.33%\n",
      "\n",
      "Train Epoch: 9 [1000/1408 (67%)]\tLoss: 0.696608, Accu: 45.00%\n",
      "\n",
      "Train set: Average loss: 0.6957, Accuracy: 48.03%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6934, Accuracy: 49.00%\n",
      "\n",
      "Epoch 9 model saved!\n",
      "Train Epoch: 10 [1000/1408 (67%)]\tLoss: 0.694321, Accu: 44.00%\n",
      "\n",
      "Train set: Average loss: 0.6931, Accuracy: 50.70%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6946, Accuracy: 50.67%\n",
      "\n",
      "Train Epoch: 11 [1000/1408 (67%)]\tLoss: 0.691680, Accu: 56.00%\n",
      "\n",
      "Train set: Average loss: 0.6932, Accuracy: 52.03%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6942, Accuracy: 49.67%\n",
      "\n",
      "Train Epoch: 12 [1000/1408 (67%)]\tLoss: 0.693532, Accu: 49.00%\n",
      "\n",
      "Train set: Average loss: 0.6943, Accuracy: 49.03%\n",
      "\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6935, Accuracy: 52.00%\n",
      "\n",
      "Train Epoch: 13 [1000/1408 (67%)]\tLoss: 0.697449, Accu: 48.00%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "                        \n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 512\n",
    "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
    "img_x, img_y = 224, 224  # resize video 2d frame size\n",
    "dropout_p_enco = 0.0      # dropout probability\n",
    "dropout_p_deco = 0.0\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "writer = SummaryWriter('runs/ff_resnet50_scratch')\n",
    "\n",
    "# training parameters\n",
    "epochs = 500        # training epochs\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "# train_overfit = []\n",
    "# for i in range(10):\n",
    "#     train_overfit.append(train_data[i])\n",
    "train_loader = data.DataLoader(train_data, **params)\n",
    "valid_loader = data.DataLoader(val_data, **params)\n",
    "\n",
    "# Create model\n",
    "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2,\n",
    "                         drop_p=dropout_p_enco, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "\n",
    "# cnn_encoder = EncoderCNN(img_x=img_x, img_y=img_y, fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2,\n",
    "#                          drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "\n",
    "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p_deco, num_classes=2).to(device)\n",
    "                         \n",
    "#cnn_encoder.load_state_dict(torch.load(os.path.join(save_model_path, 'cnn_encoder_best.pth')))\n",
    "#rnn_decoder.load_state_dict(torch.load(os.path.join(save_model_path, 'rnn_decoder_best.pth')))\n",
    "\n",
    "crnn_params = list(cnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "#optimizer = optim.SGD(crnn_params, lr = learning_rate , momentum=0.94, weight_decay = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(crnn_params, lr=1e-2)\n",
    "optimizer = torch.optim.RMSprop(crnn_params, lr=learning_rate)\n",
    "#optimizer.load_state_dict(torch.load(os.path.join(save_model_path, 'optimizer_best.pth')))\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.5)\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "# start training\n",
    "best_loss = 10\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    scheduler.step()\n",
    "    train_losses, train_scores = train_func(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation([cnn_encoder, rnn_decoder], device, optimizer, valid_loader)\n",
    "    \n",
    "    \n",
    "    ##tensorboard logs\n",
    "    writer.add_scalars(\"loss\",{'Train' : train_losses , 'Val' :epoch_test_loss}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {'Train' :train_scores, 'Val' :epoch_test_score} ,epoch)\n",
    "   \n",
    "    #lr = scheduler.get_lr()[0]\n",
    "#     tbc.save_value(\"Scheduler LR\",\"LR\", epoch,lr )\n",
    "   \n",
    "    #writer.add_scalars(\"Learning rate\", lr ,epoch)\n",
    "  \n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "    \n",
    "    ##save best model\n",
    "    if epoch_test_loss < best_loss:\n",
    "        best_loss= epoch_test_loss\n",
    "         # save Pytorch models of best record\n",
    "        torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_best_resnet50_scratch.pth'))  # save spatial_encoder\n",
    "        torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_best_resnet50_scratch.pth'))  # save motion_encoder\n",
    "        torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_best_resnet50_scratch.pth'))      # save optimizer\n",
    "        print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "    \n",
    "\n",
    "    # save all train test results\n",
    "#     A = np.array(epoch_train_losses)\n",
    "#     B = np.array(epoch_train_scores)\n",
    "#     C = np.array(epoch_test_losses)\n",
    "#     D = np.array(epoch_test_scores)\n",
    "#     np.save('./CRNN_epoch_training_losses.npy', A)\n",
    "#     np.save('./CRNN_epoch_training_scores.npy', B)\n",
    "#     np.save('./CRNN_epoch_test_loss.npy', C)\n",
    "#     np.save('./CRNN_epoch_test_score.npy', D)\n",
    "\n",
    "# # plot\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# plt.subplot(121)\n",
    "# plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n",
    "# plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "# plt.title(\"model loss\")\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('loss')\n",
    "# plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "# # 2nd figure\n",
    "# plt.subplot(122)\n",
    "# plt.plot(np.arange(1, epochs + 1), B[:, -1])  # train accuracy (on epoch end)\n",
    "# plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n",
    "# plt.title(\"training scores\")\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "# title = \"./fig_CRNN.png\"\n",
    "# plt.savefig(title, dpi=600)\n",
    "# # plt.close(fig)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet upto 20 epochs lr 1e-4 after that lr 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
